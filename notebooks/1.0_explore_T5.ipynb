{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3fed326",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "import torch\n",
    "from transformers import AutoModelWithLMHead, AutoTokenizer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9bf1e132",
   "metadata": {},
   "source": [
    "### get model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "812d3731",
   "metadata": {},
   "outputs": [],
   "source": [
    "HF_CHCKPT = \"valhalla/t5-base-squad\"\n",
    "T5_CHCKPT = \"/data/t5-large\"\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(T5_CHCKPT)\n",
    "model = AutoModelWithLMHead.from_pretrained(T5_CHCKPT)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16375bcc",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f58a9112",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6eeeeed",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_answer(question, context):\n",
    "    input_text = \"question: %s  context: %s </s>\" % (question, context)\n",
    "    features = tokenizer([input_text], return_tensors='pt')\n",
    "    print(f\"input shape: {features['input_ids'].shape}\")\n",
    "\n",
    "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "    print(f\"using: {device}\")\n",
    "    model.to(device)\n",
    "    out = model.generate(input_ids=features['input_ids'].to(device), \n",
    "                         attention_mask=features['attention_mask'].to(device))\n",
    "  \n",
    "    return tokenizer.decode(out[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a764b5e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# context = \"In Norse mythology, Valhalla is a majestic, enormous hall located in Asgard, ruled over by the god Odin.\"\n",
    "# question = \"What is Valhalla ?\"\n",
    "\n",
    "context = \"Enterprises are increasing their investments in infrastructure platforms to support Artificial Intelligence (AI) use cases and the computing needs of \\\n",
    "their data science teams. Machine Learning (ML) and Deep Learning (DL) are AI techniques that have demonstrated success across every industry vertical, including \\\n",
    "manufacturing, healthcare, retail, and cloud services. Kubeflow, a Kubernetes-native platform for ML workloads for enterprises, was released as an open-source \\\n",
    "project in December 2017. Kubeflow makes it easier to develop, deploy, and manage ML applications.\" \n",
    "\n",
    "# question = \"What is Kubeflow?\"\n",
    "question = \"What industries are AI useful for?\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51d10e7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "get_answer(question1, context1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ea7ef94",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13bf9c53",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "784a9b95",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7ce23a9",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
