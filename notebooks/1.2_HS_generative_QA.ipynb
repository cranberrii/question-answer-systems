{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8cda594a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "from typing import Dict, List, Optional, Union\n",
    "from pprint import pprint\n",
    "\n",
    "from haystack.nodes import TextConverter, PDFToTextConverter, DocxToTextConverter, PreProcessor, EmbeddingRetriever, DensePassageRetriever\n",
    "from haystack.utils import convert_files_to_docs, print_answers\n",
    "from haystack.document_stores import InMemoryDocumentStore, FAISSDocumentStore\n",
    "from haystack.nodes import FARMReader, TransformersReader, RAGenerator, Seq2SeqGenerator\n",
    "from haystack.pipelines import GenerativeQAPipeline\n",
    "from haystack.schema import Document\n",
    "\n",
    "from transformers import PreTrainedTokenizer, BatchEncoding\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3aed857f",
   "metadata": {},
   "outputs": [],
   "source": [
    "logging.basicConfig(format=\"%(levelname)s - %(name)s -  %(message)s\", level=logging.WARNING)\n",
    "logging.getLogger(\"haystack\").setLevel(logging.INFO)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee21b677",
   "metadata": {},
   "source": [
    "## get documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3152e8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "PDFS_PATH=\"/data/kg_pdfs_test/\"\n",
    "\n",
    "all_docs = convert_files_to_docs(dir_path=PDFS_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "234a860f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "018a0a99",
   "metadata": {},
   "source": [
    "## Preprocessing "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da274263",
   "metadata": {},
   "outputs": [],
   "source": [
    "preprocessor = PreProcessor(\n",
    "    clean_empty_lines=True,\n",
    "    clean_whitespace=True,\n",
    "    clean_header_footer=False,\n",
    "    split_by=\"word\",\n",
    "    split_length=128,  # smaller splits works better? \n",
    "    split_respect_sentence_boundary=True,\n",
    ")\n",
    "\n",
    "all_docs_process = preprocessor.process(all_docs)\n",
    "\n",
    "print(f\"n_files_input: {len(all_docs)}\\nn_docs_output: {len(all_docs_process)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a2e9423",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_docs_process[:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35b2bab2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "56f12716",
   "metadata": {},
   "source": [
    "## Document Store "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a294348b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# In-Memory Document Store\n",
    "# document_store = InMemoryDocumentStore()\n",
    "\n",
    "\n",
    "# The FAISSDocumentStore uses a SQL(SQLite in-memory be default) database under-the-hood to store the document text and other meta data. \n",
    "# The vector embeddings of the text are indexed on a FAISS Index that later is queried for searching answers.\n",
    "document_store = FAISSDocumentStore(sql_url = \"sqlite:///faiss_document_store_2.db\", \n",
    "                                    faiss_index_factory_str=\"Flat\", similarity=\"dot_product\", return_embedding=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bdbd9ad4",
   "metadata": {},
   "outputs": [],
   "source": [
    "document_store.write_documents(all_docs_process)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "665ddb6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "document_store.get_document_count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90109588",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "69c81719",
   "metadata": {},
   "source": [
    "## Retriever\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c36e927",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize DPR Retriever to encode documents, encode question and query documents\n",
    "\n",
    "dpr_retriever = DensePassageRetriever(\n",
    "    document_store=document_store,\n",
    "    query_embedding_model=\"facebook/dpr-question_encoder-single-nq-base\",\n",
    "    passage_embedding_model=\"facebook/dpr-ctx_encoder-single-nq-base\",\n",
    "    use_gpu=True,\n",
    "    embed_title=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f7ec859",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add documents embeddings to index\n",
    "\n",
    "document_store.update_embeddings(retriever=dpr_retriever)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7530bdca",
   "metadata": {},
   "outputs": [],
   "source": [
    "document_store.get_all_documents()[55].embedding.shape"
   ]
  },
  {
   "cell_type": "raw",
   "id": "4827cc4f",
   "metadata": {},
   "source": [
    "# Sentence BERT embeddings retriever\n",
    "SENT_TRANS_MODEL = \"sentence-transformers/multi-qa-mpnet-base-dot-v1\"\n",
    "\n",
    "embedd_ret = EmbeddingRetriever(\n",
    "    document_store=document_store,\n",
    "    embedding_model=SENT_TRANS_MODEL,\n",
    "    model_format=\"sentence_transformers\",\n",
    ")\n",
    "\n",
    "# Important:\n",
    "# Now that we initialized the Retriever, we need to call update_embeddings() to iterate over all previously indexed documents \n",
    "# and update their embedding representation.\n",
    "# While this can be a time consuming operation (depending on the corpus size), it only needs to be done once.\n",
    "# At query time, we only need to embed the query and compare it to the existing document embeddings, which is very fast.\n",
    "\n",
    "document_store.update_embeddings(embedd_ret)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cfbe9fd4",
   "metadata": {},
   "outputs": [],
   "source": [
    "query = \"What is streaming data?\"\n",
    "query1 = \"How is deep learning used in industry?\"\n",
    "query2 = \"What is a data mesh?\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e4df848",
   "metadata": {},
   "outputs": [],
   "source": [
    "dpr_ls = [(doc.content, doc.meta) for doc in dpr_retriever.retrieve(query2, top_k=5)]\n",
    "\n",
    "pprint(dpr_ls)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b46b03cf",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "55ae598f",
   "metadata": {},
   "source": [
    "## Generator"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20b6f038",
   "metadata": {},
   "source": [
    "#### Retrieval Augmented Generator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ec90833",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize RAG Generator\n",
    "\n",
    "fb_generator = RAGenerator(\n",
    "    model_name_or_path=\"facebook/rag-token-nq\",\n",
    "    use_gpu=True,\n",
    "    top_k=1,\n",
    "    max_length=100,\n",
    "    min_length=2,\n",
    "    embed_title=True,\n",
    "    num_beams=2,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ff2c85e",
   "metadata": {},
   "outputs": [],
   "source": [
    "ans = fb_generator.predict(query2, \n",
    "                           documents=dpr_retriever.retrieve(query2, top_k=5), \n",
    "                           top_k=3)\n",
    "\n",
    "# pprint(ans.get('answers'))\n",
    "pprint(ans)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "447f5167",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "dcb5fba8",
   "metadata": {},
   "source": [
    "#### T5-large"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24d15029",
   "metadata": {},
   "outputs": [],
   "source": [
    "class _T5Converter:\n",
    "    \"\"\"\n",
    "    A sequence-to-sequence model input converter (https://huggingface.co/yjernite/bart_eli5) based on the BART architecture fine-tuned on ELI5 dataset (https://arxiv.org/abs/1907.09190).\n",
    "    The converter takes documents and a query as input and formats them into a single sequence that a seq2seq model can use it as input for its generation step.\n",
    "    This includes model-specific prefixes, separation tokens and the actual conversion into tensors. \n",
    "    For more details refer to Yacine Jernite's excellent LFQA contributions at https://yjernite.github.io/lfqa.html\n",
    "    \"\"\"\n",
    "    def __call__(self, tokenizer: PreTrainedTokenizer, query: str, documents: List[Document], top_k: Optional[int] = None) -> BatchEncoding:\n",
    "        conditioned_doc = \"<P> \" + \" <P> \".join([d.content for d in documents])\n",
    "        # print(conditioned_doc)\n",
    "\n",
    "        # concatenate question and support document into BART input\n",
    "        query_and_docs = \"question: {} context: {}\".format(query, conditioned_doc)\n",
    "        max_source_length = 512\n",
    "\n",
    "        # return tokenizer([(query_and_docs, \"A\")], truncation=True, padding=True, max_length=max_source_length, return_tensors=\"pt\")\n",
    "        return tokenizer([query_and_docs], truncation=True, padding=True, max_length=max_source_length, return_tensors=\"pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3953ea6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# /data/t5-large; google/t5-large-lm-adapt\n",
    "\n",
    "t5_generator = Seq2SeqGenerator(\n",
    "    model_name_or_path=\"/data/t5-large\",\n",
    "    input_converter=_T5Converter(),\n",
    "    use_gpu=True,\n",
    "    top_k=1,\n",
    "    max_length=100,\n",
    "    min_length=2,\n",
    "    num_beams=3,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44ed5f12",
   "metadata": {},
   "outputs": [],
   "source": [
    "ans = t5_generator.predict(query2, \n",
    "                           documents=dpr_retriever.retrieve(query2, top_k=5), \n",
    "                           top_k=3)\n",
    "\n",
    "# pprint(ans.get('answers'))\n",
    "pprint(ans)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ed84fd8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "6a95144f",
   "metadata": {},
   "source": [
    "## Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87e63cf6",
   "metadata": {},
   "outputs": [],
   "source": [
    "QUESTIONS = [\n",
    "    \"What is streaming data?\",\n",
    "    \"How is deep learning used in industry?\",\n",
    "    \"What is a data mesh?\",\n",
    "    \"What do data scientists work on?\",\n",
    "    \"How can cloud storage costs be reduced?\",\n",
    "    \"What are the advantages of multi cloud?\"\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9bf929df",
   "metadata": {},
   "outputs": [],
   "source": [
    "pipe_GQA = GenerativeQAPipeline(generator=t5_generator, retriever=dpr_retriever)\n",
    "\n",
    "for question in QUESTIONS:\n",
    "    res = pipe_GQA.run(query=question, \n",
    "                       params={\"Generator\": {\"top_k\": 1}, \"Retriever\": {\"top_k\": 5}})\n",
    "    \n",
    "    (print_answers(res, details=\"all\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ca3b26a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07dcffe3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "001222a9",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
